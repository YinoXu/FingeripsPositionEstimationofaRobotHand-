{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkrIb226FVjb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.models as m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fFfAQN68nVL1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lYqXZs2tFVjd"
      },
      "outputs": [],
      "source": [
        "# Create my own image dataset class\n",
        "class MyImageDataset:\n",
        "    def __init__(self, train=True, transform=None):\n",
        "        self.transform = transform\n",
        "        if train:\n",
        "            self.pathX = \"./lazydata/train/X\"\n",
        "        else:\n",
        "            self.pathX = \"./lazydata/test/X\"\n",
        "        self.pathY = \"./lazydata/train/Y\"\n",
        "        self.data=os.listdir(self.pathX)\n",
        "        self.data=[f for f in self.data if f != \".DS_Store\"]\n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join(self.pathX, str(idx))\n",
        "        image1 = cv2.imread(os.path.join(path, \"rgb/0.png\"))\n",
        "        image2 = cv2.imread(os.path.join(path, \"rgb/1.png\"))\n",
        "        image3 = cv2.imread(os.path.join(path, \"rgb/2.png\"))\n",
        "        depth = np.load(os.path.join(path, \"depth.npy\"))\n",
        "        Y = np.load(os.path.join(self.pathY, str(idx)+\".npy\"))\n",
        "        Y *= 1000\n",
        "        if self.transform:\n",
        "            image1 = self.transform(image1)\n",
        "            image2 = self.transform(image2)\n",
        "            image3 = self.transform(image3)\n",
        "        \n",
        "        return (image1, image2, image3, depth), Y\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PH8KnufbU003"
      },
      "outputs": [],
      "source": [
        "MyImageTransformations = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([111.0820, 117.8825, 123.7023], \n",
        "                           [60.2689, 56.3253, 56.8279])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z2q7nJ-IFVjg"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, optimizer):\n",
        "    model.train()\n",
        "    loss_value = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data=torch.cat((data[0],data[1],data[2],data[3]), 1)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        model = model.to(device)\n",
        "        output = model(data)\n",
        "        \n",
        "        mse_loss = nn.MSELoss()\n",
        "        loss = mse_loss(output.float(), target.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_value += loss/149\n",
        "        if batch_idx == 148:\n",
        "          print(\"Epoch value: {} => Loss value: {}\".format(epoch,l))\n",
        "          loss_value = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = MyImageDataset(True, transform = MyImageTransformations)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USrkYgYVFVjh",
        "outputId": "b5bfc509-b696-498c-e384-e8d8341be21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch = 0, 639.6286010742188\n",
            "epoch = 1, 218.47723388671875\n",
            "epoch = 2, 127.41681671142578\n",
            "epoch = 3, 94.72064971923828\n",
            "epoch = 4, 79.39344024658203\n",
            "epoch = 5, 59.46860885620117\n",
            "epoch = 6, 48.45988082885742\n",
            "epoch = 7, 37.61295700073242\n",
            "epoch = 8, 27.82272720336914\n",
            "epoch = 9, 25.170499801635742\n",
            "epoch = 10, 21.75239372253418\n",
            "epoch = 11, 19.37546730041504\n",
            "epoch = 12, 16.516271591186523\n",
            "epoch = 13, 15.755706787109375\n",
            "epoch = 14, 14.32571029663086\n",
            "epoch = 15, 14.539135932922363\n",
            "epoch = 16, 12.382811546325684\n",
            "epoch = 17, 11.581938743591309\n",
            "epoch = 18, 11.203937530517578\n",
            "epoch = 19, 10.271881103515625\n",
            "epoch = 20, 11.463558197021484\n",
            "epoch = 21, 9.791245460510254\n",
            "epoch = 22, 8.711827278137207\n",
            "epoch = 23, 8.861493110656738\n",
            "epoch = 24, 8.074983596801758\n",
            "epoch = 25, 7.95707893371582\n",
            "epoch = 26, 8.174131393432617\n",
            "epoch = 27, 7.670015335083008\n",
            "epoch = 28, 6.956993579864502\n",
            "epoch = 29, 6.6521711349487305\n",
            "epoch = 30, 6.716496467590332\n",
            "epoch = 31, 7.025938034057617\n",
            "epoch = 32, 6.501819610595703\n",
            "epoch = 33, 5.74193000793457\n",
            "epoch = 34, 5.608610153198242\n",
            "epoch = 35, 4.899794101715088\n",
            "epoch = 36, 7.009303569793701\n",
            "epoch = 37, 5.053192138671875\n",
            "epoch = 38, 6.498754024505615\n",
            "epoch = 39, 5.204463481903076\n",
            "epoch = 40, 4.6981940269470215\n",
            "epoch = 41, 4.343085289001465\n",
            "epoch = 42, 4.004664897918701\n",
            "epoch = 43, 3.9549269676208496\n",
            "epoch = 44, 3.640042304992676\n",
            "epoch = 45, 4.220123767852783\n",
            "epoch = 46, 4.01271915435791\n",
            "epoch = 47, 4.421878337860107\n",
            "epoch = 48, 4.141008377075195\n",
            "epoch = 49, 3.6236307621002197\n"
          ]
        }
      ],
      "source": [
        "model = m.resnet50(weights=m.ResNet50_Weights.DEFAULT)\n",
        "model.eval()\n",
        "model.float()\n",
        "model.fc = nn.Linear(2048, 12)\n",
        "model.conv1 = nn.Conv2d(12, 64, kernel_size=7, stride=2, padding=3, bias=False)# with torch.no_grad():\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, momentum=0.9)\n",
        "for epoch in range(0, 50):\n",
        "    train(epoch, model, optimizer) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry74A0t5xjEV",
        "outputId": "763ccdc6-478c-4d25-d6fd-e1dab5b25b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch = 50, 3.1815264225006104\n",
            "epoch = 51, 4.088736057281494\n",
            "epoch = 52, 3.0571136474609375\n",
            "epoch = 53, 2.9833199977874756\n",
            "epoch = 54, 2.7854251861572266\n",
            "epoch = 55, 2.897265672683716\n",
            "epoch = 56, 3.195817470550537\n",
            "epoch = 57, 2.8644704818725586\n",
            "epoch = 58, 2.8478622436523438\n",
            "epoch = 59, 2.7104384899139404\n",
            "epoch = 60, 2.337191581726074\n",
            "epoch = 61, 2.7722926139831543\n",
            "epoch = 62, 2.6405677795410156\n",
            "epoch = 63, 2.508108377456665\n",
            "epoch = 64, 2.470583200454712\n",
            "epoch = 65, 2.5284721851348877\n",
            "epoch = 66, 2.841566562652588\n",
            "epoch = 67, 2.3423995971679688\n",
            "epoch = 68, 2.0501086711883545\n",
            "epoch = 69, 1.8179773092269897\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50, 70):\n",
        "    train(epoch, model, optimizer) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "B-A-UhX6FVji"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_resnet.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7qzuJUuYdv7",
        "outputId": "a14a077f-2ece-436b-cbeb-475d90a330bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written to csv file submission.csv\n"
          ]
        }
      ],
      "source": [
        "outfile = 'submission.csv'\n",
        "\n",
        "output_file = open(outfile, 'w')\n",
        "\n",
        "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
        "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
        "preds = []\n",
        "\n",
        "t_data = torch.load('./csci-ua-473-intro-to-machine-learning-fall22/test/test/testX.pt')\n",
        "file_ids = t_data[-1]\n",
        "model.eval()\n",
        "t_dataset = MyImageDataset(False, transform = MyImageTransformations)\n",
        "t_loader = torch.utils.data.DataLoader(t_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "for batch_idx, (data, target) in enumerate(t_loader):\n",
        "      data=torch.cat((data[0],data[1],data[2],data[3]), 1)\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      output = model(data)/1000\n",
        "      preds.append(output[0].cpu().detach().numpy())\n",
        "\n",
        "df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame.from_records(preds)], axis = 1, names = titles)\n",
        "df.columns = titles\n",
        "df.to_csv(outfile, index = False)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
